<div class="slide" role='region' aria-label="Slide 1 of 11" id="slide-1" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=1"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=1">Slide 1</a></noscript></div><div role='region'><p>Slide one. Hi, my name is Ann, and I'm a software engineer at Google.</p>
<p>I work on tensorflow.js, a JavaScript library for machine learning.</p>
<p>Since we launched in March of 2018, we've seen tremendous adoption by the JavaScript community with over two million downloads on <a class=dfn>NPM</a>.</p>
<p>Meanwhile, a distinctive class of machine learning applications has emerged that leverage the unique advantages of on-device computation, such as access to sensor data and preservation of user privacy.</p>
<p>In this talk, I'll discuss how <a class=dfn>TFJS</a> brings high performance, machine learning to a JavaScript through standard and emerging web technologies, including <a class=dfn>WebAssembly</a>, <a class=dfn>WebGL</a>, and <a class=dfn>WebGPU</a>.</p></div><div class="slide" role='region' aria-label="Slide 2 of 11" id="slide-2" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=2"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=2">Slide 2</a></noscript></div><div role='region'><p><a class=dfn>TFJS</a> defines an API for neural network operations, such as <a class=dfn>matrix</a> multiplication, exponentiation, etc.</p>
<p>These operations call into kernels, which implement the mathematical functions comprising the operation for a particular execution environment.</p>
<p>For example, <a class=dfn>WebAssembly</a> or <a class=dfn>WebGL</a>.</p>
<p>A <a class=dfn>TFJS</a> backend is a collection of such kernels that are defined for the same environment.</p></div><div class="slide" role='region' aria-label="Slide 3 of 11" id="slide-3" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=3"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=3">Slide 3</a></noscript></div><div role='region'><p>Before diving into the details of our different backends, I'd like to provide an overview of how they compare in terms of performance.</p>
<p>This table shows how our <a class=dfn>WebGL</a>, <a class=dfn>WebAssembly</a>, and plain JS backends compare when it comes to inference on MobileNet, a medium-sized model with a few hundred million <a class=dfn>multiply-add</a> operations.</p>
<p>For this model, our <a class=dfn>WebAssembly</a> backend is between 3 and 11 times faster than our plain JS backend.</p>
<p>Our <a class=dfn>WebAssembly</a> backend is between 5 and 8 times slower than our <a class=dfn>WebGL</a> backend.</p></div><div class="slide" role='region' aria-label="Slide 4 of 11" id="slide-4" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=4"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=4">Slide 4</a></noscript></div><div role='region'><p>This next table shows how our backends compare when it comes to inference on face detector, a much smaller model with only around 20 million <a class=dfn>multiply-add</a> operations.</p>
<p>In this case, our <a class=dfn>WebAssembly</a> backend is between 8 and 20 times faster than our plain JS backend.</p>
<p>And it's actually comparable with our <a class=dfn>WebGL</a> backend.</p>
<p>For example, on a 2018 MacBook Pro, our <a class=dfn>WebAssembly</a> backend is twice as fast as our <a class=dfn>WebGL</a> backend.</p>
<p>With <a class=dfn>SIMD</a> enabled, it's 3 times faster.</p>
<p>These benchmarks demonstrate that there is no one size-fits-all technology for machine learning on the web.</p>
<p>The best choice of execution environment depends on many factors, including the model architecture and the advice.</p>
<p>Technologies such as <a class=dfn>WebAssembly</a> and <a class=dfn>WebGL</a> address different use cases.</p>
<p>And we as <a class=dfn>TFJS</a> developers must invest in a wide swath of technologies in order to meet our users' needs.</p></div><div class="slide" role='region' aria-label="Slide 5 of 11" id="slide-5" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=5"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=5">Slide 5</a></noscript></div><div role='region'><p>Now I'll go into the details of some of our backends, starting with <a class=dfn>WebAssembly</a>.</p>
<p>Our <a class=dfn>WebAssembly</a> backend kernels are written in C++, and compiled with <a class=dfn>EMScripten</a>.</p>
<p>We use XNNPACK, a highly optimized library of neural network operators for further acceleration.</p>
<p>As we've seen, our <a class=dfn>WebAssembly</a> backend is ideally suited for lighter models.</p>
<p>And in the last year, we've seen a wave of such production quality models designed for Edge devices.</p>
<p>But <a class=dfn>WebAssembly</a> is steadily closing the performance gap with <a class=dfn>WebGL</a> for larger models as well.</p>
<p>A few weeks ago, we added support for <a class=dfn>SIMD</a> instructions to our <a class=dfn>WebAssembly</a> backend.</p>
<p>This led to a 3X performance boost for MobileNet, and a 2X performance boost for face detector.</p>
<p>We're also actively working on adding support for multithreading through SharedArrayBuffer.</p>
<p>According to our internal benchmarks, multithreading will provide an additional 3x performance boost for MobileNet, and 2x performance boost for face detector.</p></div><div class="slide" role='region' aria-label="Slide 6 of 11" id="slide-6" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=6"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=6">Slide 6</a></noscript></div><div role='region'><p>For these reasons, we expect adoption of our <a class=dfn>WebAssembly</a> backend to continue to grow.</p>
<p>We're eager for more users to enjoy the benefits of <a class=dfn>SIMD</a> and multithreading.</p>
<p>We're also closely following the progress of several evolving specifications in <a class=dfn>WebAssembly</a>, including flexible vectors for Wider <a class=dfn>SIMD</a>, quasi-fused <a class=dfn>multiply-add</a>, and pseudo minimum and maximum instructions.</p>
<p>We're also looking forward to ES6 module support for <a class=dfn>WebAssembly</a> modules.</p></div><div class="slide" role='region' aria-label="Slide 7 of 11" id="slide-7" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=7"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=7">Slide 7</a></noscript></div><div role='region'><p><a class=dfn>TFJS</a> also offers a GPU accelerated backend built on top of the <a class=dfn>WebGL</a> API. We repurpose this API for high performance numerical computation by representing data in the form of GPU textures, and using fragment shaders to execute neural network operations.</p>
<p>As we've seen, our <a class=dfn>WebGL</a> backend is still the fastest for larger models containing wide operations that justify the fixed overhead costs of shader execution.</p></div><div class="slide" role='region' aria-label="Slide 8 of 11" id="slide-8" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=8"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=8">Slide 8</a></noscript></div><div role='region'><p>Our <a class=dfn>WebGL</a> backend is complex.</p>
<p>This complexity comes from many sources.</p>
<p>Firstly, <a class=dfn>WebGL</a> implementations vary significantly across platforms, often with implications for numerical precision.</p>
<p>Much of our code is devoted to hiding these inconsistencies from our users.</p>
<p>Another significant source of complexity in our <a class=dfn>WebGL</a> backend is manual memory management.</p>
<p>Because GPU resources are not garbage collected, we must explicitly manage resource disposal through reference counting.</p>
<p>To help our users avoid leaking memory, we expose a utility called tf.tidy that takes a function, executes it, and disposes any intermediate resources created by that function.</p>
<p>Despite these measures, memory management remains a source of error in our <a class=dfn>WebGL</a> backend.</p>
<p>Therefore, we're excited about new proposals for user-defined finalizers that would give us more security against memory leaks.</p>
<p>Finally, the lack of callbacks for asynchronous <a class=dfn>WebGL</a> texture downloading means we must pull for download completion.</p>
<p>This has implications for both code complexity and performance.</p></div><div class="slide" role='region' aria-label="Slide 9 of 11" id="slide-9" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=9"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=9">Slide 9</a></noscript></div><div role='region'><p>TFJs also offers an experimental backend built on top of the emerging <a class=dfn>WebGPU</a> standard.</p>
<p><a class=dfn>WebGPU</a> represents an exciting opportunity for addressing the pain points of <a class=dfn>WebGL</a>.</p>
<p><a class=dfn>WebGPU</a> promises better performance and a dedicated API for GPU compute.</p>
<p>As the successor to <a class=dfn>WebGL</a>, <a class=dfn>WebGPU</a> is designed to operate directly with low-level graphics APIs, such as D3D, Metal, and Vulkan.</p>
<p>The <a class=dfn>WebGPU</a> shading language will be directly ingested, and will hopefully offer faster shader compilation, compared to <a class=dfn>WebGL</a>.</p></div><div class="slide" role='region' aria-label="Slide 10 of 11" id="slide-10" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=10"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=10">Slide 10</a></noscript></div><div role='region'><p>This table shows inference speeds for the PO's net model, built on top of the resonant 50 architecture, a large model with several billion <a class=dfn>multiply-add</a> operations.</p>
<p>These benchmarks demonstrate the reality that <a class=dfn>WebGPU</a> has not delivered significant out-of-the-box performance gains.</p>
<p>However, the technology is rapidly evolving, and we're continuing to track progress closely.</p></div><div class="slide" role='region' aria-label="Slide 11 of 11" id="slide-11" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=11"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ay_tensorflow.pdf#page=11">Slide 11</a></noscript></div><div role='region'><p>We're excited about the potential for future machine learning web standards to address the recurring pain points we faced in developing <a class=dfn>TFJS</a>, including lack of portability and manual memory management.</p>
<p>Such standards also represent an opportunity to address the distinctive needs of machine learning-powered web applications.</p>
<p>For example, <a class=dfn>TFJS</a> users have increasingly asked for ways to obfuscate their models in order to protect intellectual property.</p>
<p>We also hope that future standards will preserve the features that have made our progress thus far possible, such as detailed profiling and access to low-level APIs that give us the ability to define and debug operations at a granular level.</p>
<p>Alright, that's it for me.</p>
<p>Thank you very much.</p></div>