<div class="slide" role='region' aria-label="Slide 1 of 10" id="slide-1" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=1"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=1">Slide 1</a></noscript></div><div role='region'><p>I’m Bernard Aboba, presenting on behalf of the Chairs of the <a class=dfn>WebRTC</a> Working Group: myself, Harald Alvestrand and Jan-Ivar Bruaroey on the topic of Machine Learning and Web Media.</p></div><div class="slide" role='region' aria-label="Slide 2 of 10" id="slide-2" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=2"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=2">Slide 2</a></noscript></div><div role='region'><p>The Pandemic of 2020 has been a pivotal moment for the world, with many institutions experiencing unprecedented stress.</p>
<p>Amidst all the tragedy, we have seen an unparalleled level of user-driven innovation as consumers and businesses struggle to survive and perhaps even thrive, leading to a decade’s worth of innovation in only a few months, in areas as diverse as politics, art, entertainment and sports.</p>
<p>Yogi Berra said, “you can observe a lot, just by watching”.</p>
<p>What have you observed?</p>
<p>Here are some of the things in my scrapbook.</p></div><div class="slide" role='region' aria-label="Slide 3 of 10" id="slide-3" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=3"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=3">Slide 3</a></noscript></div><div role='region'><p>Live theatre has been particularly hard hit during the pandemic.</p>
<p>Rather than cancelling their 2020-2021 season, Tacoma Little Theatre has chosen to move it online.</p>
<p>Their first online production, of “Robin Hood”, utilized custom backgrounds for scenery, combining conferencing (for the live production) with Youtube for archiving and subsequent streaming.</p>
<p>Typically, custom backgrounds are implemented using machine learning algorithms that locally operate on captured video, extracting human forms which are then overlaid on the selected backgrounds before being encoded for transmission.</p></div><div class="slide" role='region' aria-label="Slide 4 of 10" id="slide-4" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=4"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=4">Slide 4</a></noscript></div><div role='region'><p>During the pandemic, sporting matches have had to be conducted in the absence of fans.</p>
<p>To incorporate fans back into the game, the NBA collaborated with Microsoft to introduce “Together Mode” where video from fans are combined so as to make it seem like they are viewing the game courtside.</p>
<p>As with the previous example, this one involves machine learning algorithms operating locally on captured video prior to encoding and transmission to a server producing a composite video including both the fans as well as the game.</p></div><div class="slide" role='region' aria-label="Slide 5 of 10" id="slide-5" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=5"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=5">Slide 5</a></noscript></div><div role='region'><p>Machine learning algorithms in these use cases operate on captured media  prior to encoding or transmission.</p>
<p>For audio, machine learning can be used for noise suppression, and for video it might provide for background removal, “together mode”, or “funny hats”.</p>
<p>Processing may also occur on a centralized server acting as a receiver, such as production of a composite video.</p>
<p>On the local system, one proposal for obtaining access to raw video is to add a method on a <a class=dfn>MediaStreamTrack</a>.</p></div><div class="slide" role='region' aria-label="Slide 6 of 10" id="slide-6" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=6"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=6">Slide 6</a></noscript></div><div role='region'><p>APIs that provide access to encoded media such as <a class=dfn>Insertable Streams</a> and <a class=dfn>WebCodecs</a> operate at a different place in the pipeline, after encoding (on the sender) or prior to decode (on the receiver).</p>
<p>Since raw video is considerably larger than encoded video, and the processing can be quite different, the performance of a Transform stream operating on encoded media within <a class=dfn>Insertable Streams</a> may not be sufficient for machine learning algorithms operating on raw media.</p></div><div class="slide" role='region' aria-label="Slide 7 of 10" id="slide-7" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=7"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=7">Slide 7</a></noscript></div><div role='region'><p>Performance across the pipeline is an important requirement: capture, application of machine learning models, encoding and transmission.</p>
<p>It is desirable for each pipeline stage to operate on buffers provided by the previous stage, avoiding memory copies.</p>
<p>For example, if the capture device can provide buffers to the machine learning algorithm (perhaps a GPU buffer?) without extraneous copies.</p>
<p>After processing by machine learning algorithms, <a class=dfn>WebCodecs</a> would encode and WebTransport would perform network I/O, also ideally without extraneous copies.</p></div><div class="slide" role='region' aria-label="Slide 8 of 10" id="slide-8" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=8"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=8">Slide 8</a></noscript></div><div role='region'><p>The <a class=dfn>Insertable Streams</a> and <a class=dfn>WebCodecs</a> proposals share data structures such as the representation of audio and video frames.</p>
<p>So even though <a class=dfn>WebCodecs</a> does not use the <a class=dfn>WHATWG Streams API</a> used by <a class=dfn>Insertable Streams</a> and WebTransport, <a class=dfn>WebCodecs</a> is still leveraging the <a class=dfn>Insertable Streams</a> implementation experience.</p></div><div class="slide" role='region' aria-label="Slide 9 of 10" id="slide-9" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=9"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=9">Slide 9</a></noscript></div><div role='region'><p>A few words about <a class=dfn>AV1</a> support in realtime communications.</p>
<p><a class=dfn>AV1</a> offers not only improved compression efficiency but also a new image format (AVIF) likely to be widely adopted in browsers.</p>
<p>The higher encoding complexity of <a class=dfn>AV1</a> makes performance considerations particularly important, but there are proposals to enable software encoders to be practical before hardware acceleration becomes widely available.</p>
<p>These include capability advertisement to allows <a class=dfn>AV1</a> to be only used for decode; mixed-codec simulcast, allowing <a class=dfn>AV1</a> to be used to encode low bitrate video with other codecs used for higher bitrate encodings; content-hints, allowing <a class=dfn>AV1</a> to be used for screen content coding (often at low framerates), and scalable video coding.</p></div><div class="slide" role='region' aria-label="Slide 10 of 10" id="slide-10" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=10"><noscript><a href="https://www.w3.org/2020/Talks/mlws/ba-media.pdf#page=10">Slide 10</a></noscript></div><div role='region'><p>Thank you for listening to this presentation.</p>
<p>In addition to the participants and Chairs of the <a class=dfn>WebRTC</a> Working Group we would like to thank Dom who provided feedback on the presentation and helped put it together.</p></div>