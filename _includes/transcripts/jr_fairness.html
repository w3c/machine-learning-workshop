<div class="slide" role='region' aria-label="Slide 1 of 27" id="slide-1" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=1"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=1">Slide 1</a></noscript></div><div role='region'><p>Hi, this is John Rochford.</p>
<p>It's my honor and privilege to present to you today.</p>
<p>My talk title is AI Machine Learning: Bias and Garbage In, Bias and Garbage Out.</p>
<p>I am the INDEX program director and a faculty member at the Eunice Kennedy Shriver Center, part of UMass Medical School.</p>
<p>I'm an Invited Expert for the World Wide Web Consortium.</p></div><div class="slide" role='region' aria-label="Slide 2 of 27" id="slide-2" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=2"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=2">Slide 2</a></noscript></div><div role='region'><p>A little about me, I am legally blind.</p>
<p>I trade privacy for utility.</p>
<p>The 14 Alexas in my home enable me to control devices with interfaces I can't see.</p>
<p>Hey Alexa, what time is it now?</p>
<p>I am tired of telling you what time it is.</p>
<p>Look at a clock.</p>
<p>What's the heck Alexa!</p>
<p>And of course, she records to the cloud everything I say.</p>
<p>I'm an AI researcher focused on ML text simplification, ML fairness and empowerment and I'm a 35-year developer of technology for the disabled.</p></div><div class="slide" role='region' aria-label="Slide 3 of 27" id="slide-3" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=3"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=3">Slide 3</a></noscript></div><div role='region'><p>These are some of the INDEX staff.</p>
<p>We are from all over the world.</p>
<p>We are speakers of 16 languages.</p>
<p>We are women.</p>
<p>We are the disabled.</p></div><div class="slide" role='region' aria-label="Slide 4 of 27" id="slide-4" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=4"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=4">Slide 4</a></noscript></div><div role='region'><p>A big INDEX project is creating easy-to-understand COVID-19 information worldwide.</p>
<p>With our easy Text.AI startup, INDEX ML engineers and disabled staff are developing our ML model and using crowdsourcing to simplify COVID-19 texts from the websites of every country's governments.</p>
<p>Simple COVID-19 texts will help huge populations of people with cognitive disabilities, low literacy, non-native language speakers and seniors make the world safe and healthy.</p></div><div class="slide" role='region' aria-label="Slide 5 of 27" id="slide-5" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=5"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=5">Slide 5</a></noscript></div><div role='region'><p>My presentation targets are ML Social Bias, why we should care, what we should do about it, how we can do it.</p>
<p>And I hope to persuade you that for your machine learning efforts to be successful, their development and data must include disabled people.</p></div><div class="slide" role='region' aria-label="Slide 6 of 27" id="slide-6" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=6"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=6">Slide 6</a></noscript></div><div role='region'><p>But first, a shout out to Black Lives Matter.</p>
<p>University of Toronto and MIT researchers found that every facial recognition system they tested performed better on lighter-skinned faces.</p>
<p>That includes a one-in-three failure rate with identifying darker-skinned females.</p>
<p>This is from the article, Gender Shades, Intersectional Accuracy Disparities and Commercial Gender Classification.</p></div><div class="slide" role='region' aria-label="Slide 7 of 27" id="slide-7" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=7"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=7">Slide 7</a></noscript></div><div role='region'><p>The author of that article, Joy, has a TED talk.</p>
<p>It's called How I'm Fighting Bias in Algorithms.</p>
<p>And what she's showing here in the picture is that for her face to be recognized, she has to wear a mask of a white female.</p></div><div class="slide" role='region' aria-label="Slide 8 of 27" id="slide-8" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=8"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=8">Slide 8</a></noscript></div><div role='region'><p>Why care?</p>
<p>Profound responsibility.</p>
<p>“There's nothing artificial about AI.</p>
<p>It's inspired by people, it's created by people and most importantly, it impacts people.</p>
<p>It is a powerful tool we are only just beginning to understand and that is a profound responsibility.” From the article Bias in, Bias out, the Stanford Scientist Out to Make AI Less White and Male.</p></div><div class="slide" role='region' aria-label="Slide 9 of 27" id="slide-9" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=9"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=9">Slide 9</a></noscript></div><div role='region'><p>Why care about machine learning fairness and disability?</p>
<p>It's the most significant challenge of our time.</p>
<p>If it doesn't work for all, nobody will trust it.</p>
<p>The disabled are part of every segment of society.</p>
<p>Thus, if we can solve the problem for the disabled, we can solve the problem for everyone.</p></div><div class="slide" role='region' aria-label="Slide 10 of 27" id="slide-10" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=10"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=10">Slide 10</a></noscript></div><div role='region'><p>Why care?</p>
<p>What about you?</p>
<p>“Bias in ML has been almost ubiquitous when the application is involved in people and it has already hurt the benefit of people in minority groups or historically disadvantageous groups.</p>
<p>Not only people in minority groups but everyone should care about the bias in AI.</p>
<p>If no one cares, it is highly likely that the next person who suffers from bias treatment is one of us.” From the article, A Tutorial in Fairness in machine learning.</p></div><div class="slide" role='region' aria-label="Slide 11 of 27" id="slide-11" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=11"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=11">Slide 11</a></noscript></div><div role='region'><p>Being smart can be a deficit in ML.</p>
<p>“People who perform better on a test of pattern detection, a measure of cognitive ability, are also quicker to form and apply stereotypes.” “In other words, being smart might put you at a greater risk of prejudice but you can still fight against those instincts by challenging your thinking and getting to know people who aren't like you.” This is from the article, Smart People Are More Likely To Stereotype.</p></div><div class="slide" role='region' aria-label="Slide 12 of 27" id="slide-12" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=12"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=12">Slide 12</a></noscript></div><div role='region'><p>How can you get to know people who aren't like you?</p>
<p>Include the disabled in machine learning development.</p>
<p>“To ensure AI-based systems are treating people with disabilities fairly, it is essential to include them in the development process.” This is from the article, How to tackle AI Bias For People With Disabilities.</p></div><div class="slide" role='region' aria-label="Slide 13 of 27" id="slide-13" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=13"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=13">Slide 13</a></noscript></div><div role='region'><p>Here's an MIT failure to include the disabled.</p>
<p>An MIT project claimed to translate American Sign Language with machine learning and sign-language gloves.</p>
<p>It failed because sign language is much more than communicating with hands.</p>
<p>It's about body language and facial expressions.</p>
<p>MIT would have known that was essential had it involved the Deaf community.</p>
<p>From the article, Why Sign-Language Gloves Don't Help Deaf People.</p></div><div class="slide" role='region' aria-label="Slide 14 of 27" id="slide-14" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=14"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=14">Slide 14</a></noscript></div><div role='region'><p>A big issue is lack of data from the disabled due to privacy concerns.</p>
<p>Many of us do not disclose our disabilities.</p>
<p>When we do, we are denied employment, housing, and more.</p>
<p>Thus, despite being 15% of the population, we are significantly underrepresented in training data.</p></div><div class="slide" role='region' aria-label="Slide 15 of 27" id="slide-15" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=15"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=15">Slide 15</a></noscript></div><div role='region'><p>What can we do about a lack of data from the disabled?</p>
<p>Accurate analysis.</p>
<p>“We are helping machine learning engineers, figure out what questions to ask of their data, to diagnose why their systems may be making unfair predictions.” This is from the article, MIT Researchers Show How To Detect and Address AI Bias Without Loss in Accuracy.</p></div><div class="slide" role='region' aria-label="Slide 16 of 27" id="slide-16" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=16"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=16">Slide 16</a></noscript></div><div role='region'><p>What can we do with data we do have from the disabled?</p>
<p>Fairness through unawareness: “No information about protected attributes, e.g gender, age, disability is gathered and used in the decision-making.</p>
<p>Fairness through awareness: Membership in a protected group is explicitly known and fairness can be formally defined, tested, and enforced algorithmically.” This is from the article, AI Fairness For People With Disabilities Point of View.</p></div><div class="slide" role='region' aria-label="Slide 17 of 27" id="slide-17" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=17"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=17">Slide 17</a></noscript></div><div role='region'><p>What can we do to mitigate our lack of data from the disabled?</p>
<p>One possibility is to use synthetic data.</p>
<p>“Given a data set involving a protected attribute with a privileged and unprivileged group, we create an ideal world data set.</p>
<p>For every data sample, we create a new sample having the same features except the protected attributes and label as the original sample but with the opposite protected attribute value.” This is from the article, Data Augmentation for Discrimination Prevention and Bias Disambiguation.</p></div><div class="slide" role='region' aria-label="Slide 18 of 27" id="slide-18" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=18"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=18">Slide 18</a></noscript></div><div role='region'><p>What tools will help us implement machine learning fairness?</p>
<p>Well there are commercial tools.</p>
<p>Many companies such as Microsoft, Amazon, IBM offer bias mitigation services in their ML platforms.</p>
<p>There are open source tools.</p>
<p>There are an increasing number on GitHub.</p>
<p>The following are the best of the current ones.</p></div><div class="slide" role='region' aria-label="Slide 19 of 27" id="slide-19" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=19"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=19">Slide 19</a></noscript></div><div role='region'><p>The Google What-If Tool visually probes the behavior of trained machine learning models with minimal coding.</p>
<p>This interface looks pretty cool to me.</p>
<p>But then again, I'm blind.</p></div><div class="slide" role='region' aria-label="Slide 20 of 27" id="slide-20" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=20"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=20">Slide 20</a></noscript></div><div role='region'><p>Fairlearn, a Python package to assess and improve fairness of machine learning models.</p></div><div class="slide" role='region' aria-label="Slide 21 of 27" id="slide-21" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=21"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=21">Slide 21</a></noscript></div><div role='region'><p>Skater, a Python library designed to demystify the learned structures of a black box model, both globally (inference on the basis of a complete dataset) and locally (inference about an individual prediction).</p></div><div class="slide" role='region' aria-label="Slide 22 of 27" id="slide-22" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=22"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=22">Slide 22</a></noscript></div><div role='region'><p>Pymetrics.</p>
<p>It detects demographic differences and the output of machine learning models or other assessments.</p></div><div class="slide" role='region' aria-label="Slide 23 of 27" id="slide-23" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=23"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=23">Slide 23</a></noscript></div><div role='region'><p>IBM AI Fairness 360 Open Source Toolkit.</p>
<p>A comprehensive set of fairness metrics for dataset machine learning models, explanations for these metrics, and algorithms to mitigate bias in data sets and models.</p></div><div class="slide" role='region' aria-label="Slide 24 of 27" id="slide-24" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=24"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=24">Slide 24</a></noscript></div><div role='region'><p>IBM AI Factsheets 360.</p>
<p>A research effort to foster trust in AI by increasing transparency and enabling governance.</p></div><div class="slide" role='region' aria-label="Slide 25 of 27" id="slide-25" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=25"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=25">Slide 25</a></noscript></div><div role='region'><p>Final thought about disability.</p>
<p>There is a saying we shout from the rooftops for all to hear.</p>
<p>I hope you do.</p>
<p>“Nothing about us without us”.</p></div><div class="slide" role='region' aria-label="Slide 26 of 27" id="slide-26" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=26"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=26">Slide 26</a></noscript></div><div role='region'><p>A final thought for you, you need to be woke if you want your AI to be woke.</p></div><div class="slide" role='region' aria-label="Slide 27 of 27" id="slide-27" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=27"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jr_fairness.pdf#page=27">Slide 27</a></noscript></div><div role='region'><p>Here's my contact info.</p>
<p>There's a QR code for my presentation and a link to it.</p>
<p>There's another link to AI Fairness resources.</p>
<p>I thank you for listening to the little I know about machine learning fairness.</p></div>