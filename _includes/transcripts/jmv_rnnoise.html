<div class="slide" role='region' aria-label="Slide 1 of 7" id="slide-1" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=1"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=1">Slide 1</a></noscript></div><div role='region'><p>Hi, I'm Jean-Marc Valin, I'm the author of RNNoise.</p>
<p>And I'll be talking about neural speech enhancement, through RNNoise, and also about how it affects the browser.</p>
<p>I'm currently employed by Amazon, but I'm giving this talk as an individual.</p></div><div class="slide" role='region' aria-label="Slide 2 of 7" id="slide-2" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=2"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=2">Slide 2</a></noscript></div><div role='region'><p>Speech enhancement isn't exactly a new topic.</p>
<p>It's been around since the 70s, and traditionally it's done using signal processing.</p>
<p>It uses complicated spectral estimators, usually combined with hand-tune parameters.</p>
<p>And it works pretty decently on stationary noise, at mid to higher SNRs.</p>
<p>The complexity is very low, but the quality is limited.</p>
<p>On the other hand, there's a new approach, which is based on deep neural networks.</p>
<p>It's entirely data driven, so no need to tune all these parameters.</p>
<p>But they use large models, so typically in the 10s of megabytes, and it handles non stationary noise that works at low <a class=dfn>SNR</a>, so much higher quality than the traditional approach.</p>
<p>But unfortunately, the complexity is quite high.</p>
<p>And RNNoise is a way of trying to get the best of both worlds.</p>
<p>So, trying to get to the same quality as <a class=dfn>DNN</a> approach with the complexity of the <a class=dfn>DSP</a> approach.</p></div><div class="slide" role='region' aria-label="Slide 3 of 7" id="slide-3" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=3"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=3">Slide 3</a></noscript></div><div role='region'><p>RNNoise is really a hybrid solution.</p>
<p>It starts from a conventional <a class=dfn>DSP</a> approach, and from there, it replaces these complicated estimators with a deep neural network, that includes several fully connected layer as well as three <a class=dfn>GRU</a> layers.</p>
<p>One of the key tricks to help bring the complexity down, is that the spectrum is divided into 22 critical bands, rather than processing every single frequency bin separately.</p>
<p>And each of these 22 bands is independently attenuated.</p>
<p>So we have a gain for each of these bands, and it controls how each band is modulated.</p>
<p>This works pretty well except for one case, when we have voice speech and we have noise between pitch harmonics.</p>
<p>And to handle that case, we have a pitch filter that acts as a comb filter and removes the noise between the harmonics, to get actual clean speech.</p></div><div class="slide" role='region' aria-label="Slide 4 of 7" id="slide-4" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=4"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=4">Slide 4</a></noscript></div><div role='region'><p>(fan humming) In terms of results, you can hear here, the effects of RNNoise being toggled on and off, while I'm speaking and typing at the same time with a fan in the background.</p>
<p>You can observe the result in this slide, they are based on test evaluation, so objective evaluation.</p>
<p>And you can also go to this interactive demo, where you can listen to several samples, and also actually try RNNoise on your own voice using JavaScript.</p></div><div class="slide" role='region' aria-label="Slide 5 of 7" id="slide-5" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=5"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=5">Slide 5</a></noscript></div><div role='region'><p>Now let's look at the complexity of RNNoise, for a 48 kilohertz mono input signal.</p>
<p>RNNoise uses 215 neurons, which means 88,000 weights, and it processes audio in frames of 10 milliseconds, which means we have 100 frames per second.</p>
<p>The total complexity in RNNoise is around 40 <a class=dfn>megaflops</a>.</p>
<p>And the most complex parts, are first the <a class=dfn>DNN</a>, which is mostly made of <a class=dfn>matrix</a>-vector products.</p>
<p>And the complexity of that, is around 17 and a half <a class=dfn>megaflops</a>.</p>
<p>We have FFTs and IFFTs, and those costs around seven and a half <a class=dfn>megaflops</a>.</p>
<p>And then we have a pitch search, which uses a correlation or <a class=dfn>convolution</a> and cost around 10 <a class=dfn>megaflops</a>.</p>
<p>But these are the main parts.</p>
<p>So, if we wanna optimize RNNoise, then these are the things we need to look at.</p>
<p>The current code base you can find on GitHub, is C code, completely unoptimized, not vectorized.</p>
<p>And it still runs with about 1% CPU on X86, about 40% on a Raspberry Pi 3, and we even have a version that runs in real-time in the browser, through EMscripten and JavaScript.</p></div><div class="slide" role='region' aria-label="Slide 6 of 7" id="slide-6" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=6"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=6">Slide 6</a></noscript></div><div role='region'><p>Looking forward a bit, RNNoise is really a minimalistic solution, its <a class=dfn>DNN</a> is really quite small compared to other approaches.</p>
<p>But in the future, you could see systems where it would grow by a factor of 100 or even 1000.</p>
<p>It is mostly made of <a class=dfn>matrix</a>-vector products, especially if we grow the <a class=dfn>DNN</a>, the <a class=dfn>FFT</a> will become negligible.</p>
<p>And so if we want it to run in real time, we need low overhead because we need many of these <a class=dfn>matrix</a>-vector products every second.</p>
<p>In terms of pure <a class=dfn>DNN</a> approaches, some of them are using really large <a class=dfn>convolutional network</a>.</p>
<p>And that involves complexity sometimes up to the 10th of <a class=dfn>gigaflops</a>, which may even require GPUs in some cases, if we wanted to run in real time.</p>
<p>And there's also a new approach that is emerging, it's not yet clear what will become of this, but these are <a class=dfn>vocoder</a>-based re-synthesis approaches, where the idea is to de-noise acoustic features, rather than audio, and then use a <a class=dfn>TTS</a> like <a class=dfn>vocoder</a> to resynthesize clean speech out of this.</p>
<p>So it could potentially provide much fewer artifacts in the denoised speech.</p>
<p>And if we want that to run in real time, the most promising approaches are through WaveRNN or even LPCNet.</p>
<p>Those involve around 3 to 10 <a class=dfn>gigaflops</a>, so less than some of the pure <a class=dfn>DNN</a> approaches, but at the same time, it requires a processing of the sample level, which means that many GPUs will not be able to process that in real time, and we will actually need a CPU because we need to compute the network for every single sample at 16 or 24 or 48 KHz in the future.</p></div><div class="slide" role='region' aria-label="Slide 7 of 7" id="slide-7" data-fmt="pdf" data-src="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=7"><noscript><a href="https://www.w3.org/2020/Talks/mlws/jmv_rnnoise.pdf#page=7">Slide 7</a></noscript></div><div role='region'><p>That concludes my talk, for those interested, the RNNoise source code is available on GitHub under a BSD license.</p>
<p>You can also have a look at the demo page for many samples as well as some high level explanation.</p>
<p>And you can have a look at some of the references here if you want to read more about RNNoise and some of the topics for this talk.</p>
<p>Thank you.</p></div>