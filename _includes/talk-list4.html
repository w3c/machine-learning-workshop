<dl class="talks"><dt>We Count: Fair Treatment, Disability and Machine Learning</dt><dd><dl><dt>Speaker</dt><dd>Jutta Treviranus (OCAD University)</dd><dd>    Director & Professor, Inclusive Design Research Centre, OCAD University</dd><dt>Speaker</dt><dd><p>The risks of AI Bias have recently received attention in public discourse. Numerous stories of the automation and amplification of existing discrimination and inequity are emerging, as more and more critical decisions and functions are handed over to machine learning systems. There is a growing movement to tackle non-representative data and to prevent the introduction of human biases into machine learning algorithms.</p><p>However, these efforts are not addressing a fundamental characteristic of data driven decisions that presents significant risk if you have a disability. Even if there is full proportional representation and even if all human bias is removed from AI systems, the systems will favour the majority and dominant patterns. This has implications for individuals and groups that are outliers, small minorities or highly heterogeneous. The only common characteristic of disability is sufficient difference from the average such that most systems are a misfit and present a barrier. Machine learning requires large data sets. Many people with disabilities represent a data set of one. Decisions based on population data will decide against small minorities and for the majority. The further you are from average the harder it will be to train machine learning systems to serve your needs. To add insult to injury, if you are an outlier and highly unique, privacy protections won’t work for you and you will be most vulnerable to data abuse and misuse.</p><p>This presentation will:<ul><li>       outline the risks and opportunities presented by machine learning systems;<li>       address strategies to mitigate the risks; and<li>       discuss steps needed to support decisions that do not discriminate against outliers and small minorities.</ul><p>The benefits for innovation and the well-being of society as a whole will also be discussed</dd></dl></dd>
<dt>AI (Machine Learning): Bias & Garbage In, Bias & Garbage Out</dt><dd><dl><dt>Speaker</dt><dd>John Rochford (University of Massachusetts Medical School)</dd><dd>Director, INDEX Program, Eunice Kennedy Shriver Center, University of Massachusetts Medical School</dd><dt>Speaker</dt><dd>Biased training data produces untrustworthy, unfair, useless results. Such results include:<ul><li>   predicting black prisoners are the most likely recidivist; and<li>   killing a wheelchair user in a street crosswalk by autonomous car ML models.</ul><p>   Training data must include representation of people with disabilities, all races, all ethnicities, all genders, etc. Creation of training data must include those populations. There are opensource and commercial toolkits and APIs to facilitate bias mitigation.</p>         <p>John is an expert in this area focused on AI fairness and empowerment for people with disabilities and is a member of the Machine Learning for the Web Community Group.</dd></dl></dd>
<dt>Interactive ML - Powered Music Applications on the Web</dt><dd><dl><dt>Speaker</dt><dd>Tero Parviainen (Counterpoint)</dd><dd>    Tero Parviainen is a software developer in music, media, and the arts. As a co-founder of creative technology studio Counterpoint, he's recently built installations for The Barbican Centre, Somerset House, The Helsinki Festival, The Dallas Museum of Art, and various corners of the web. He also contributes at Wavepaths, building generative music systems for psychedelic therapy.</dd><dt>Speaker</dt><dd>    This talk will present a few projects Counterpoint has built with TensorFlow.js and Magenta.js over the past couple of years. Ranging from experimental musical instruments to interactive artworks, they've really stretched what can be done in the browser context. It will focus on the special considerations needed in music & audio applications, the relationship between ML models and Web Audio, and the limitations encountered while combining the two.</dd></dl></dd>
<dt>Wreck a Nice Beach in the Browser: Getting the Browser to Recognize Speech</dt><dd><dl><dt>Speaker</dt><dd>Kelly Davis (Mozilla)</dd><dd>    Manager of the machine learning group at Mozilla. Kelly's work at Mozilla includes Deep Speech (an open speech recognition system), Common Voice (a crowdsourced tool for creating opens speech corpora), Mozilla's TTS (an open source speech synthesis system), Snakepit (an open source ML job scheduler), as well as ML research and many other projects.</dd></dl></dd>
<dt>Challenges to create great user experience using natural language processing model in B2B SaaS product on the web</dt><dd><dl><dt>Speaker</dt><dd>Ryuichi Tanimoto (Stockmark)</dd><dd>    web developer/system architect at Stockmark Inc.</dd><dt>Speaker</dt><dd>    Stockmark develops a B2B SaaS product based on machine learning and natural language processing technology to provide search and analysis UX of business news on the web and support the market research or competitive analysis in the business scene. This talk will share Stockmark's experience with developing the machine learning processes with fastText or BERT model in browser that suggests certain news or phrases or quantitative information in the text of searched news, in response to user's action to the web page.</dd></dl></dd>
<dt>Privacy focussed machine translation in Firefox</dt><dd><dl><dt>Speaker</dt><dd>Nikolay Bogoychev (University of Edinburgh)</dd><dd>    Postdoc researcher at the University of Edinburgh</dd><dt>Speaker</dt><dd>    In the recent years, machine translation has been widely adopted by the end user, making online content in foreign languages more accessible than ever. However, machine translation has always been treated as a computationally heavy problem and as such is usually delivered to the end user via online services such as Google Translate, which may not be appropriate for sensitive content. We present a privacy focussed machine translation system that runs locally on the user's machine and is accessible through a Firefox browser extension. The translation models used are just 16MB and translation speed is high enough for a seamless user experience even on laptops from 2012.</dd></dl></dd>
<dt>A virtual character web meeting with expression enhance power by machine learning</dt><dd><dl><dt>Speaker</dt><dd>Zelun Chen (Netease)</dd><dd>    Front-end and Client Development Engineer of Netease</dd><dt>Speaker</dt><dd>    This talk will cover the use of machine learning to enhance participant's expression in a virtual character web meeting and highlight the problems of using webassembly to running AI models In browser.</dd></dl></dd>
<dt>RNNoise, Neural Speech Enhancement, and the Browser</dt><dd><dl><dt>Speaker</dt><dd>Jean-Marc Valin</dd><dd>    Jean-Marc Valin has previously contributed to the Opus and AV1 codecs. He is employed by Amazon, but is giving this talk as an individual.</dd><dt>Speaker</dt><dd>     This talk presents RNNoise, a small and fast real-time noise suppression algorithm that combines classical signal processing with deep learning. We will discuss the algorithm and how the browser can be improved to make RNNoise and other neural speech enhancement algorithms more efficient.</dd></dl></dd>
<dt>Empowering Musicians and Artists using Machine Learning to Build Their Own Tools in the Browser</dt><dd><dl><dt>Speaker</dt><dd>Louis McCallum (University of London)</dd><dd>Louis  is an experienced software developer, researcher, artist and musician. Currently, he holds a Post Doctoral position at the Embodied AudioVisual Interaction Group, Goldsmiths, University of London, where he is also an Associate Lecturer. He is also lead developer on the <a href='https://mimicproject.com'>MIMIC platform</a> and accompanying <a href='https://mimicproject.com/guides/learner'>Learner.js</a> and <a href='https://mimicproject.com/guides/maxi-instrument'>MaxiInstrument.js</a> libraries</dd><dt>Speaker</dt><dd>Over the past 2 years, as part of the RCUK AHRC funded <a href='http://mimicproject.com/'>MIMIC project</a> we have provided platforms and libraries for musicians and artists to use, perform and collaborate online using machine learning. Although it has a lot to offer these communities, their skill sets and requirements often diverge from more conventional machine learning use cases. For example, requirements for dynamic provision of data and on-the-fly training in the browser raises challenges with performance, connectivity and storage. We seek to address the non trivial challenges of connecting inputs from a variety of sources, running potentially computationally expensive feature extractors alongside lightweight machine learning models and generating audio and visual output, in real time, without interference. Whilst technologies like AudioWorklets addresses this to some extent, there remain issues with implementation, documentation and adoption (currently limited to Chrome). For example, issues with <a href='https://bugs.chromium.org/p/chromium/issues/detail?id=1033493#c42'>garbage collection (created by the worker thread messaging system)</a> caused wide scale disruption to many developers using AudioWorklets and was only addressed by a <a href='https://github.com/padenot/ringbuf.js'>ringbuffer solution</a> that developers must integrate outside of the core API. We are also keen to ensure the <a href='https://gpuweb.github.io/gpuweb/'>WebGPU API</a>  takes realtime media into consideration as it is introduced. Our talk will cover both the user’s perspectives as uncovered by our user-centered research and a developer’s perspective from the technical challenges we have faced developing tools to meet the needs of these users in both creative and educational settings.</dd></dl></dd>
</dl>